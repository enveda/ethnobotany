{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Relation extraction on BERN2 data\n",
    "\n",
    "Processing steps:\n",
    "1. Download BERN2 data: http://nlp.dmis.korea.edu/projects/bern2-sung-et-al-2022/annotation_v1.1.tar.gz\n",
    "2. Process the files in the form of **sentence, plant_mention, disease_mention**. The original file has the PubMed identifier, sentence number (which can also be obtained using a sentence_splitter), the entity mentions and their CURIEs (MeSH and NCBITaxon identifiers). Since the BERN2 dump is over 60GB cannot be run in a laptop, the easiest way to process the file is to remove all the entities that are not MeSH or NCBITaxon identifiers, and proceed from there to format it as described above (**sentence, plant_mention, disease_mention**).\n",
    "3. Load a dataframe and and run notebook (**run_relation_extraction** method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Dict, Optional, Any, Tuple\n",
    "\n",
    "from thefuzz import fuzz\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BASE_MODEL = \"bigscience/T0_11B\"\n",
    "HEAVIEST_MODEL = \"bigscience/T0pp\"\n",
    "\n",
    "def initialize_model(huggingface_model: str = BASE_MODEL) -> Tuple[AutoModelForSeq2SeqLM, AutoTokenizer]:\n",
    "    \"\"\"Initialize model and tokenizer.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(huggingface_model)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(huggingface_model)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def _process_answer(answer: str) -> str:\n",
    "    \"\"\"Process answer from model.\"\"\"\n",
    "    # This requires to remove special characters, stripping and lower casing\n",
    "    return answer.replace('</s>', '').replace('<pad>', '').strip().lower()\n",
    "\n",
    "\n",
    "def run_prompt(model, tokenizer, prompt):\n",
    "    \"\"\"Run prompt on the model.\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        outputs = model.generate(inputs)\n",
    "    except Exception as e:\n",
    "        logger.error(f'skip sentence: {e} \\n prompt {prompt}')\n",
    "        return ''\n",
    "\n",
    "    # Return the answer after processing\n",
    "    return _process_answer(tokenizer.decode(outputs[0]))\n",
    "\n",
    "\n",
    "def _evaluate_plant_disease_prompt(\n",
    "    plant_entity: str,\n",
    "    disease_entity: str,\n",
    "    answer_1: str,\n",
    "    answer_2: str,\n",
    "    answer_3: str,\n",
    "):\n",
    "    \"\"\"Evaluate the answers of the model for a plant-disease prompt.\"\"\"\n",
    "    answers = []\n",
    "\n",
    "    # Evaluate answer 1 (which plants are used to treat {disease_entity}?)\n",
    "    if plant_entity in answer_1 or fuzz.partial_ratio(answer_1, plant_entity) > 90:\n",
    "        answers.append(True)\n",
    "    else:\n",
    "        answers.append(False)\n",
    "\n",
    "    # Evaluate answer 2 (is {plant_entity} used to treat {disease_entity}?)\n",
    "    if 'true' in answer_2:\n",
    "        answers.append(True)\n",
    "    else:\n",
    "        answers.append(False)\n",
    "\n",
    "    # Evaluate answer 3 (which diseases are associated with {plant_entity}?)\n",
    "    if disease_entity in answer_3 or fuzz.partial_ratio(answer_3, disease_entity) > 90:\n",
    "        answers.append(True)\n",
    "    else:\n",
    "        answers.append(False)\n",
    "\n",
    "    # If any of the answers if True\n",
    "    if any(answers):\n",
    "        edge_exist = True\n",
    "\n",
    "        if all(answers):\n",
    "            confidence = 'high'\n",
    "        elif sum(answers) == 2:\n",
    "            confidence = 'medium'\n",
    "        else:\n",
    "            confidence = 'low'\n",
    "    else:\n",
    "        edge_exist = False\n",
    "        confidence = ''\n",
    "\n",
    "    return {\n",
    "        'edge_exists': edge_exist,\n",
    "        'confidence': confidence,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_relation_extraction(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    sentence: str,\n",
    "    plant_mention: str,\n",
    "    disease_mention: str,\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Run a prompt of the language model to evaluate whether there is a relation between a plant and a disease.\"\"\"\n",
    "\n",
    "    plant_entity = plant_mention.strip()\n",
    "    disease_entity = disease_mention.strip()\n",
    "\n",
    "    answer_1 = run_prompt(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=f\"{sentence}. In the previous sentence, which plants are used to treat {disease_entity}?\"\n",
    "    )\n",
    "\n",
    "    answer_2 = run_prompt(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=f\"{sentence}. In the previous sentence, is {plant_entity} used to treat {disease_entity}?\"\n",
    "    )\n",
    "\n",
    "    answer_3 = run_prompt(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=f\"{sentence}. In the previous sentence, which diseases are associated with {plant_entity}?\"\n",
    "    )\n",
    "\n",
    "    return _evaluate_plant_disease_prompt(\n",
    "        plant_entity=plant_entity,\n",
    "        disease_entity=disease_entity,\n",
    "        answer_1=answer_1,\n",
    "        answer_2=answer_2,\n",
    "        answer_3=answer_3,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}